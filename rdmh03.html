<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  CommonHTML: { matchFontHeight: false }
});
</script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
<style>  
	p {  
		text-indent: 1.0em;  
	}  
</style>  

</head>

<body>
<h1>Counting Statistics and Error Prediction</h1>
Radioactive decay is a random process. Consequently, any measurement based on observing
the radiation emitted in nuclear decay is subject to some degree of statistical fluctuation.
These inherent fluctuations represent an unavoidable source of uncertainty in all
nuclear measurements and often can be the predominant source of imprecision or error.
The term counting statistics includes the framework of statistical analysis required to
process the results of nuclear counting experiments and to make predictions about the
expected precision of quantities derived from these measurements.

<p>The value of counting statistics falls into two general categories. The first is to serve as
a check on the normal functioning of a piece of nuclear counting equipment. Here a set of
measurements is recorded under conditions in which all aspects of the experiment are held
as constant as possible. Because of the influence of statistical fluctuations, these measurements
will not all be the same but will show some degree of internal variation. The amount
of this fluctuation can be quantified and compared with predictions of statistical models. If
the amount of observed fluctuation is not consistent with predictions, one can conclude
that some abnormality exists in the counting system. The second application is generally
more valuable and deals with the situation in which we have only one measurement. We
can then use counting statistics to predict its inherent statistical uncertainty and thus estimate
an accuracy that should be associated with that single measurement.</p>

<p>The distinctions made in the organization of this chapter are a critical part of the topic.
The confusion that often arises when the student is first introduced to counting statistics
originates more from a failure to keep separate the concepts presented in Sections I and II
than from any other single cause. In Section I we are careful to limit the discussion to methods
used in the characterization or organization of experimental data. We are not particularly
concerned where these data come from but rather are interested only in presenting
the formal methods by which we can describe the amount of fluctuation displayed by the
data. In Section II, we discuss the separate topic of probabilistic mathematical models,
which can sometimes represent real measurement systems. For purposes of the discussion
in Section II, however, we are concerned only with the structure and predictions of these
models as mathematical entities. We reserve, until Section III, the demonstration of how
the statistical models can be matched to experimental data, resulting in the two common
applications of counting statistics mentioned above. In Section IV, we examine how the
predicted statistical uncertainties propagate through the calculations typically needed to
produce a quoted final result that is calculated from counting data. The final three sections
of the chapter illustrate some further examples of applications of statistical principles in
radiation measurements.</p>

<h2>I. CHARACTERIZATION OF DATA</h2>
We begin by assuming that we have a collection of N independent measurements of the
same physical quantity:
\[
x_{1}, x_{2}, x_{3}, ... , x_{i}, ... , x_{N}
\]

We further assume that a single typical value xi from this set can only assume integer values
so that the data might represent, for example, a number of successive readings from a
radiation counter for repeated time intervals of equal length. Two elementary properties of
this data set are

\[
Sum \qquad \Sigma \equiv \sum_{i = 1}^{N}x_{i} \tag{3.1}
\]

\[
Experimental \quad mean \qquad \overline{x}_{e} \equiv \Sigma / N \tag{3.2}
\]

The experimental mean is written with the subscript to distinguish it from the mean of a
particular statistical model that will be introduced later.
<p>It is often convenient to represent the data set by a corresponding <i>frequency distribution
function</i> \(F(x)\). The value of \(F(x)\) is the relative frequency with which the number
appears in the collection of data. By definition
\[
F(x) = 
\frac
	{
		\begin{eqnarray}
			\rm number \; of \; occurrences \; of \; the \; value \; 
		\end{eqnarray} x
	}
	{
		\begin{eqnarray}
			\rm number \; of \; measurements \; 
		\end{eqnarray}
		(= N)
	} \tag{3.3}
\]

The distribution is automatically normalized, that is,

\[
\sum_{x = 0}^{\infty}F(x) = 1 \tag{3.4}
\]

As long as we do not care about the specific sequence of the numbers, the complete data
distribution function \(F(x)\) represents all the information contained in the original data set.</p>

<img src="tab3.1.png" width="454" height="386" alt="">

<p>For purposes of illustration, Table 3.1 gives a hypothetical set of data consisting of 20
entries. Because these entries range from a minimum of 3 to maximum of 14, the data distribution
function will have nonzero values only between these extreme values of the
argument x. The corresponding values of \(F(x)\) are also shown in Table 3.1.</p>

<p>A plot of the data distribution function for the example is given in Fig. 3.1. Also shown
directly above the plot is a horizontal bar graph of the original 20 numbers from which the
distribution was derived. These data show an experimental mean of 8.8, and the distribution
function is in some sense centered about that value. Furthermore, the relative shape of the
distribution function indicates qualitatively the amount of internal fluctuation in the data set.
For example, Fig. 3.2 shows the shape of the distribution functions corresponding to two
extreme sets of data: one with large amounts of scatter about the mean and one with little
scatter. An obvious conclusion is that the width of the distribution function is a relative measure
of the amount of fluctuation or scattering about the mean inherent in a given set of data.</p>

<p>It is possible to calculate the experimental mean by using the data distribution function,
because the mean of any distribution is simply its first moment
\[
\overline{x}_{e} = \sum_{x = 0}^{\infty}xF(x) \tag{3.5}
\]

It is also possible to derive another parameter, called the <i>sample variance</i>, which will serve
to quantify the amount of internal fluctuation in the data set. The first step is to define the</p>

<p>
<img src="fig3.1.png" width="408" height="574" alt="">
<img src="fig3.2.png" width="536" height="422" alt="">
</p>

<i>residual</i> of any data point as the amount by which it differs from the experimental mean
value

\[
d_{i} \equiv x_{i} - \overline{x}_{e} \tag{3.6}
\]

To illustrate, the example of the 20 numbers given in Table 3.1 is shown as the bar graph of
Fig. 3 . 3 ~T.h e individual residuals of these values have been separately plotted in part (b)
of the figure. There must be an equal contribution of positive and negative residuals, so that

\[
\sum_{i = 1}^{N}	d_{i} = 0
\]

<p>
<img src="fig3.3.png" width="578" height="341" alt="">
</p>

If we take the square of each residual, however, a positive number will always result. These
are plotted for the example in Fig. 3.3~.

<p>We next define the <i>deviation</i> of a given data point as the amount by which it differs
from the true mean value \(\overline{x}\)</p>

\[
\epsilon_{i} \equiv x_{i} - \overline{x} \tag{3.7}
\]

The deviation defined in this way is similar to the residual introduced above, except that
the distance from the true mean value \(\overline{x}\) appears in the definition rather than the experimental
mean \(\overline{x}_{e}\). We now can introduce the definition of the sample variance as the average
value of each of these deviations after squaring

\[
s^{2} = \overline{\epsilon^{2}} = \frac{1}{N}\sum_{i = 1}^{N}(x_{i} - \overline{x})^{2} \tag{3.8}
\]

The sample variance is a useful index of the degree of the internal scatter in the data or as
a measure of how different a typical number is from another.

<p>This definition presents a practical difficulty, since we can never know the exact value of
the true mean \(\overline{x}\) without collecting an infinite number of data points. The best we can do is to
use the experimental mean value \(\overline{x}_{e}\) that we have measured, and thus use rcsiduals rather
than deviations. But the process of using the experimental rather than the true mean value
will affect the calculated value of the sample variance, and we cannot simply substitute \(\overline{x}_{e}\) iein to
Eq. 3.8. Instead, the analysis given in Appendix B shows that the alternative expression</p>

\[
s^{2} = \frac{1}{N - 1}\sum_{i = 1}^{N}(x_{i} - \overline{x})^{2} \tag{3.9}
\]

is now valid when the experimental mean is used. The sum of squared residuals in the
above equation is divided by \(N - 1\) rather than by \(N\) as in Eq. 3.8, a distinction that is significant
only when the number of measurements \(N\) is small. For large data sets, therefore,
the sample variance can be thought of as the mean squared value of either the residuals or
the deviations.

<p>The sample variance \(s^{2}\) for the example of 20 numbers is shown graphically in Fig. 3.3~.
Because it is essentially a measure of the average value of the squared deviations of each
point, \(s^{2}\) is an effective measure of the amount of fluctuation in the original data. A data
set with a narrow distribution will have a small typical deviation from the mean, and therefore
the value for the sample variance will be small. On the other hand, data with a large
amount of fluctuation will have a wide distribution and a large value for typical deviations,
and the corresponding sample variance will also be large. It is important to note that the
sample variance is an absolute measure of the amount of internal scatter in the data and
does not, to first approximation, depend on the number of values in the data set. For example,
if the data shown in Fig. 3.3 were extended by simply collecting an additional 20 values
by the same process, we would not expect the sample variance calculated for the extended
collection of 40 numbers to be substantially different from that shown in Fig 3.3.</p>

<p>We can also calculate the sample variance directly from the data distribution function
 \(F(x)\). Because Eq. (3.8) indicates that \(s^{2}\) is simply the average of \((x - \overline{x})^{2}\), we can write that
same average as</p>

\[
s^{2} = \sum_{x = 0}^{\infty}(x - \overline{x})^{2}F(x) \tag{3.10}
\]

Equation (3.10) is not introduced so much for its usefulness in computation as for the parallel
it provides to a similar expression, Eq. (3.17), which will be introduced in a later discussion
of statistical models. An expansion of Eq. (3.10) will yield the well-known result

\[
s^{2} = \overline{x^{2}} - (\overline{x})^{2} \tag{3.11}
\]

<p>We now end our discussion of the organization of experimental data with two important
conclusions:</p>

<ol type="1">
<li>Any set of data can be completely described by its frequency distribution function \(F(x)\).</li>
<li>Two properties of this frequency distribution function are of particular interest: the
experimental mean and the sample variance.</li>
</ol>

<p>The experimental mean is given by Eq. (3.5) and is the value about which the distribution
is centered.The sample variance is given by Eq. (3.10) and is a measure of the width
of the distribution, or the amount of internal fluctuation in the data.</p>

<h2>II. STATISTICAL MODELS</h2>
Under certain conditions, we can predict the distribution function that will describe the 
results of many repetitions of a given measurement. We define a measurement as counting 
the number of successes resulting from a given number of <i>trials</i>. Each trial is assumed to be 
a <i>binary</i> process in that only two results are possible: Either the trial is a success or it is not 
a success. For everything that follows, we also assume that the probability of success is a 
constant for all trials.

<p>To show how these conditions apply to real situations, Table 3.2 gives three separate
examples. The third example indicates the basis for applying the theoretical framework
that follows to the case of counting nuclear radiation events. In this case a trial consists of
observing a given radioactive nucleus for a period of time <i>t</i>, the number of trials is equivalent
to the number of nuclei in the sample under observation, and the measurement consists
of counting those nuclei that undergo decay. We identify the probability of success of
any one trial asp. In the case of radioactive decay, that probability is equal to \((1 - e^{- \lambda t})\).
where \(\lambda\) is the decay constant of the radioactive sample.</p>

<p>Three specific statistical models are introduced:
<ol type="1">
<li>
<i>The Binomial Distribution</i>. This is the most general model and is widely applicable
to all constant-<i>p</i> processes. It is, unfortunately, computationally cumbersome in
radioactive decay, where the number of nuclei is always very large, and is used only
rarely in nuclear counting applications.
</li>
<li>
<i>The Poisson Distribution</i>. This model is a direct mathematical simplification of the
binomial distribution under conditions that the success probability <i>p</i> is small and
constant. In practical terms, this condition implies that we have chosen an observation
time that is small compared with the half-life of the source. Then the number of
radioactive nuclei remains essentially constant during the observation, and the probability
of recording a count from a given nucleus in the sample is small.
</li>
<li>
<i>The Gaussian or Normal Distribution</i>. The third important distribution is the
Gaussian, which is a further simplification if the average number of successes is relatively
large (say greater than 20 or 30). This condition will apply for any situation in
which we accumulate more than a few counts during the course of the measurement. This 
is most often the case so that the Gaussian model is widely applicable to many
problems in counting statistics.
</li>
</ol>
<img src="tab3.2.png" width="651" height="182" alt="">
</p>

<p>It should be emphasized that all the above models become identical for processes with
a small individual success probability <i>p</i> but with a large enough number of trials so that the
expected mean number of successes is large.</p>

<h3>A. The Binomial Distribution</h3>
The binomial distribution is the most general of the statistical models discussed. If <i>n</i> is the
number of trials for which each trial has a success probability <i>p</i>, then the predicted probability
of counting exactly <i>x</i> successes can be shown to be

\[
P(x) = \frac{n!}{(n-x)!x!}p^{x}(1 - p)^{n - p} \tag{3.12}
\]

<i>P(x)</i> is the predicted probability distribution function, as given by the binomial distribution,
and is defined only for integer values of <i>n</i> and <i>x</i>.

<p>We show one example of an application of the binomial distribution. Imagine that we
have an honest die so that the numbers 1 through 6 are all equally probable. Let us define
a successful roll as one in which any of the numbers 3,4,5,o r 6 appear. Because these are
four of the six possible results, the individual probability of success <i>p</i> is equal to \(\frac{4}{6}\) or 0.667.
We now roll a die a total of 10 times and record the number of rolls that result in success
as defined above. The binomial distribution now allows us to calculate the probability that
exactly <i>x</i> out of the 10 trials will be successful, where <i>x</i> can vary between 0 and 10. Table
3.3 gives the values of the predicted probability distribution from Eq. (3.12) for the parameters 
\(p = \frac{2}{3}\) and <i>n</i> = 10. The results are also plotted in Fig. 3.4 .We see that 7 is the most
probable number of successes from 10 rolls of the die, with a probability of occurrence
slightly greater than one out of four. From the value of <i>P(0)</i> we see that only twice out of
100,000 tests would we expect to see no successes from 10 rolls of the die.</p>
<img src="tab3.3.png" width="329" height="434" alt="">
<img src="fig3.4.png" width="451" height="440" alt="">

<p>Some properties of the binomial distribution are important. First, the distribution is
normalized:
\[
\sum_{x = 0}^{n}P(x) = 1 \tag{3.13}
\]

Also, we know that the average or mean value of the distribution is given by
\[
\overline{x} = \sum_{x = 0}^{n}xP(x) \tag{3.14}
\]

If we now substitute Eq. (3.12) for <i>P(x)</i> and carry out the summation, a remarkably simple
result is derived:
\[
\overline{x} = pn \tag{3.15}
\]

Thus, we can calculate the expected average number of successes by multiplying the number
of trials n by the probability p that any one trial will result in a success. In the example
just discussed, we calculate an average number of successes as
\[
\overline{x} = pn = \left( \frac{2}{3} \right) (10) = 6.67 \tag{3.16}
\]

The mean value is obviously a very fundamental and important property of any predicted
distribution.</p>

<p>It is also important to derive a single parameter that can describe the amount of fluctuation
predicted by a given distribution. We have already defined such a parameter, called
the sample variance, for a set of experimental data as defined in Eq. (3.10). By analogy we
now define a <i>predicted variance</i> \(\sigma^{2}\), which is a measure of the scatter about the mean predicted
by a specific statistical model <i>P(x)</i>:
\[
\sigma^{2} = \sum_{x = 0}^{n}(x - \overline{x})^{2}P(x) \tag{3.17}
\]

Conventionally, \(\sigma^{2}\) is called the variance, and we emphasize the fact that it is associated
with a predicted probability distribution function by calling it apredicted varimce. It is also
conventional to define the standard deviation as the square root of \(\sigma^{2}\). Recall that the variance
is in some sense a typical value of the squared deviation from the mean. Therefore, \(\sigma\) 
represents a typical value for the deviation itself, hence the name "standard deviation."</p>

<p>Now if we carry out the summation indicated in Eq. (3.17) for the specific case of <i>P(x)</i>
given by the binomial distribution, the following result is obtained:
\[
\sigma^{2} = np(1 - p) \tag{3.18}
\]

Because \(\overline{x} = np\), we can also write
\[
\sigma^{2} = \overline{x}(1 - p) \tag{3.19}
\]
\[
\sigma = \sqrt{\overline{x}(1 - p)} \tag{3.20}
\]

We now have an expression that gives an immediate prediction of the amount of fluctuation
inherent in a given binomial distribution in terms of the basic parameters of the distribution,
<i>n</i> and <i>p</i>, where \(\overline{x} = np\).</p>

<p>To return to the example of rolling a die, we defined success in such a way that \(p = \frac{2}{3}\).
We also assumed 10 rolls of the die for each measurement so that <i>n</i> = 10. For this example,
the predicted mean number of successes is 6.67 and we can proceed to calculate the
predicted variance

\[
\sigma^{2} = np(1 - p) = (10)(0.667)(0.333) = 2.22 \tag{3.21}
\]

By taking the square root we get the predicted standard deviation:

\[
\sigma = \sqrt{\sigma} = \sqrt{2.22} = 1.49 \tag{3.22}
\]

The significance of the standard deviation is illustrated in Fig. 3.4 .The mean value of the
distribution is shown as the dashed line, and one value of the standard deviation is shown
on either side of this mean. Because \(\sigma\) is a typical value for the difference between a given
measurement and the true value of the mean, wide distributions will have large values for
a and narrow distributions will correspond to small values. The plot illustrates that the
association of \(\sigma\) with the width of the distribution is not inconsistent with the example
shown in Fig. 3.4.</p>

<h3>B. The Poisson Distribution</h3>
Many categories of binary processes can be characterized by a constant and small proba-
bility of success for each individual trial. Included are most nuclear counting experiments 
in which the number of nuclei in the sample is large and the observation time is short com-
pared with the half-life of the radioactive species. Similarly, in a particle beam experiment, 
many particles from an accelerator might strike a target for every recorded reaction prod-
uct. Under these conditions, the approximation holds that the success probability is small 
and constant, and the binomial distribution reduces to the Poisson form shown below. For 
circumstances in which the observation is not short compared with the half-life of the 
source, the simple Poisson shown below no longer holds and a modified Poisson distribu-
tion, outlined in Appendix C, will apply.

<p>It can be shown that for a constant and small probability of success, the binomial distribution 
reduces to the form
\[
P(x) = \frac{(pn)^{x}e^{-pn}}{x!} \tag{3.23}
\]

Because \(pn = \overline{x}\) holds for this distribution as well as for the parent binomial distribution,
\[
P(x) = \frac{(\overline{x})^{x}e^{- \overline{x}}}{x!} \tag{3.24}
\]
which is now the familiar form of the Poisson distribution.</p>

<p>Recall that the binomial distribution requires values for two parameters: the number of
trials <i>n</i> and the individual success probability <i>p</i>. We note from Eq. (3.24) that a significant
simplification has occurred in deriving the Poisson distribution &mdash; only one parameter is
required, which is the product of <i>n</i> and <i>p</i>. This is a very useful simplification because now we
need only know the mean value of the distribution in order to reconstruct its amplitude at all
other values of the argument. That is a great help for processes in which we can in some way
measure or estimate the mean value, but for which we have no idea of either the individual
probability or the size of the sample. Such is usually the case in nuclear measurements.</p>

<p>Some properties of the Poisson distribution follow directly. First, it is also a normalized
distribution, or
\[
\sum_{x = 0}^{n}P(x) = 1 \tag{3.25}
\]

We can also calculate the first moment or mean value of the distribution:
\[
\overline{x} = \sum_{x = 0}^{n}xP(x) = pn \tag{3.26}
\]

which is the intuitively obvious result also obtained for the binomial distribution. The predicted
variance of the distribution, however, differs from that of the binomial and can be
evaluated from our prior definition
\[
\sigma^{2} = \sum_{x = 0}^{n}(x - \overline{x})^{2}P(x) = pn \tag{3.27}
\]

or noting the result from Eq. (3.26)
\[
\sigma^{2} = \overline{x} \tag{3.28}
\]

The predicted standard deviation is just the square root of the predicted variance, or
\[
\sigma = \sqrt{\overline{x}} \tag{3.29}
\]</p>

<p>Thus, we see that the predicted standard deviation of any Poisson distribution is just the
square root of the mean value that characterizes that same distribution. Note that the corresponding
result obtained earlier for the binomial distribution [Eq. (3.20)] reduces to the
above result in the limit of \(p \ll 1\) already incorporated into the Poisson assumptions.</p>

<p>We again illustrate with an example. Suppose we randomly select a group of 1000 people
and define our measurement as counting the number of current birthdays found among
all members of that group. The measurement then consists of 1000 trials, each of which is a
success only if a particular individual has his or her birthday today. If we assume a random
distribution of birthdays, the probability of success p is equal to 1/365. Because <i>p</i> is much
less than one in this example, we can immediately turn to the Poisson distribution to evaluate
the probability distribution function that will describe the expected results from many
such samplings of 1000 people. Thus, for our example,
\[
p = 1 / 365 = 0.00274 \\
n = 1000 \\
\overline{x} = pn = 2.74 \\
\sigma = \sqrt{\overline{x}} = 1.66 \\
P(x) = \frac{\overline{x}^{x}e^{-\overline{x}}}{x!} = \frac{(2.74)^xe^{-2.74}}{x!} 
\]

<img src="fig3.5.png" width="407" height="287" alt="">
<img src="noname1.png" width="158" height="282" alt=""></p>

<p>Recall that <i>P(x)</i> gives the predicted probability that exactly <i>x</i> birthdays will be observed
from a random sampling of 1000 people. The numerical values are plotted in Fig. 3.5 and show
that <i>x</i> = 2 is the most probable result. The mean value of 2.74 is also shown in the figure,
together with one value of the standard deviation of 1.66 on either side of the mean. The distribution
is roughly centered about the mean value, although considerable asymmetry is evident
for this low value of the mean. Again the size of the standard deviation gives some indication
of the width of the distribution or the amount of scatter predicted by the distribution.</p>

<h3>C. The Gaussian or Normal Distribution</h3>
The Poisson distribution holds as a mathematical simplification to the binomial distribution
in the limit \(p \ll 1\). If, in addition, the mean value of the distribution is large (say
greater than 20), additional simplifications can generally be carried out which lead to the
Gaussian distribution:
\[
P(x) = \frac{1}{\sqrt{2 \pi \overline{x}}}\exp \left( - \frac{(x - \overline{x})^{2}}{2 \overline{x}} \right) \tag{3.30}
\]

This is again a pointwise distribution function defined only for integer values of x. It shares
the following properties with the Poisson distribution:

<ol type="1">
<li>It is normalized: \(\sum_{x = 0}^{\infty}P(x) = 1\).</li>
<li>The distribution is characterized by a single parameter \(\overline{x}\), which is given by the product <i>np</i>.</li>
<li>The predicted variance \(\sigma^{2}\) as defined in Eq. (3.17) is again equal to the mean value \(\overline{x}\).</li>
</ol>

<img src="fig3.6.png" width="541" height="504" alt="">

<p>We can again illustrate an example of a physical situation in which the Gaussian distribution
is applicable. Suppose we return to the previous example of counting birthdays
out of a group of randomly selected individuals, but now consider a much larger group of
10,000 people. For this example, \(p = \frac{1}{365}\) and \(n = 10,000\), so the predicted mean value of the
distribution \(\overline{x} = np = 27.4\). Because the predicted mean is larger than 20, we can turn to the
Gaussian distribution for the predicted distribution of the results of many measurements,
each of which consists of counting the number of birthdays found in a different group of
10,000 people. The predicted probability of observing a specific count <i>x</i> is then given by
\[
P(x) = \frac{1}{\sqrt{2 \pi \cdot 27.4}}\exp \left( - \frac{(x - 27.4)^2}{2 \cdot 27.4} \right) \tag{3.31}
\]

and the predicted standard deviation for the example is
\[
\sigma = \sqrt{\overline{x}} = \sqrt{27.4} = 5.23 \tag{3.32}
\]
The results are shown graphically in Fig. 3.6a.</p>

<p>Two important observations can be made at this point about the Gaussian distribution:
<ol type="1">
<li>
The distribution is symmetric about the mean value \(\overline{x}\). Therefore, <i>P(x)</i> depends only
on the absolute value of the deviation of any value <i>x</i> from the mean, defined as 
 \(\epsilon \equiv |x - \overline{x}|\).
</li>
<li>
Because the mean value \(\overline{x}\) is large, values of <i>P(x)</i> for adjacent values of <i>x</i> are not
greatly different from each other. In other words, the distribution is slowly varying.
</li>
</ol>
These two observations suggest a recasting of the distribution as an explicit function of the
deviation \(\epsilon\) (rather than of <i>x</i>) and as a continuous function (rather than a pointwise discrete
function). These changes are accomplished by rewriting the Gaussian distribution as
\[
G(\epsilon) = \sqrt{\frac{2}{\pi \overline{x}}}e^{- \epsilon^{2}/2 \overline{x}} \tag{3.33}
\]

where \(G(\epsilon)d \epsilon\) is now defined as the differential probability of observing a deviation in \(d \epsilon\)
about \(\epsilon\). Comparing Eq. (3.33) with Eq. (3.30), we note a factor of 2 that has entered in \(G(\epsilon)\) 
because there are two values of <i>x</i> for every value of the deviation \(\epsilon\).</p>

<p>Figure 3.6b shows the continuous form of the Gaussian distribution for the same example
chosen to illustrate the discrete case. Comparing Fig. 3.6a and 3.6b, the scale factors for
each abscissa are the same but the origin for Fig. 3.6b has been shifted to illustrate that a
value of zero for the deviation \(\epsilon\) corresponds to the position of the mean value \(\overline{x}\) on
Fig. 3.6a. If a factor of 2 difference in the relative ordinate scale is included as shown, then
the continuous distribution \(G(\epsilon)\) represents the smooth curve that connects the pointwise
values plotted in Fig. 3.6a.</p>

<p>Because we are now dealing with a continuous function, we must redefine some properties
of the distribution as shown in Fig. 3.7. It should be particularly noted that quantities
of physical interest now involve integrals of the distribution between set limits, or areas
under the curve, rather than sums of discrete values.</p>

<p>Equation (3.33) can be rewritten in a more general form by incorporating several
observations. We have already seen that the standard deviation \(\sigma\) of a Gaussian distribution
is given by \(\sigma = \sqrt{\overline{x}}\), or \(\overline{x} = \sigma^{2}\). With this substitution in Eq. (3.33), the value of the
exponential factor now depends only on the ratio of \(\epsilon\) tp \(\sigma\). Formally defining this ratio as
\[
t \equiv \frac{\epsilon}{\sigma}
\]

the Gaussian distribution can be rewritten in terms of this new variable <i>t</i>:
\[
G(t) = G(\epsilon)\frac{d \epsilon}{dt} = G(\epsilon)\sigma
\]
\[
G(t) = \sqrt{\frac{2}{\pi}}e^{-t^{2}/2} \tag{3.34}
\]

where \(0 \leq t \leq \infty\). We now have a universal form of the Gaussian distribution, shown in
Fig. 3.8, that is valid for all values of the mean \(\overline{x}\). Recall that <i>t</i> is just the observed deviation
\(\epsilon \equiv |x - \overline{x}|\) normalized in units of the standard deviation \(\sigma\).</p>

<img src="fig3.7.png" width="574" height="353" alt="">
<img src="fig3.8.png" width="322" height="369" alt="">

<p>From the definitions illustrated in Fig. 3.7, the probability that a typical normalized
deviation <i>t</i> predicted by a Gaussian distribution will be less than a specific value \(t_{0}\) is given
by the integral
\[
\int_{0}^{t_{0}}G(t)dt \equiv f(t_{0})
\]
<img src="tab3.4.png" width="368" height="347" alt=""></p>

Tabulated values of this function can be found in most collections of statistical tables. Some
selected values are given in Table 3.4. The value of \(f(t_{0})\) gives the probability that a random
sample from a Gaussian distribution will show a normalized deviation \(t\)(\(\equiv \epsilon/\sigma\)) that is less
than the assumed value to. For example, we can conclude that about 68% of all samples will
deviate from the true mean by less than one value of the standard deviation.

<h2>III. APPLICATIONS OF STATISTICAL MODELS</h2>












</body>
</html>